{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pysolr\n",
      "  Downloading pysolr-3.10.0.tar.gz (59 kB)\n",
      "     ---------------------------------------- 0.0/59.1 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/59.1 kB ? eta -:--:--\n",
      "     -------------------------------- ----- 51.2/59.1 kB 650.2 kB/s eta 0:00:01\n",
      "     -------------------------------------- 59.1/59.1 kB 626.5 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: requests>=2.9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pysolr) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pysolr) (69.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.9.1->pysolr) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.9.1->pysolr) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.9.1->pysolr) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.9.1->pysolr) (2024.6.2)\n",
      "Building wheels for collected packages: pysolr\n",
      "  Building wheel for pysolr (pyproject.toml): started\n",
      "  Building wheel for pysolr (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pysolr: filename=pysolr-3.10.0-py2.py3-none-any.whl size=20095 sha256=d033f479f9464ef811389c504de7107334110603d2e923b4dc6696b3ab3c395b\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\b8\\7f\\b7\\1526b5945096311831a68d98bb9ec407725cf087a858bbc272\n",
      "Successfully built pysolr\n",
      "Installing collected packages: pysolr\n",
      "Successfully installed pysolr-3.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pysolr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Department': ['Engineering'], 'Gender': ['Male'], 'Ethnicity': ['Asian'], 'Age': [47], 'Country': ['United States'], 'City': ['Columbus'], 'id': '80d33121-4502-4533-8679-17d46524ed64', 'Employee_ID': ['E02002'], 'Full_Name': ['Kai Le'], 'Job_Title': ['Controls Engineer'], 'Business_Unit': ['Manufacturing'], 'Hire_Date': ['2/5/2022'], 'Annual_Salary': ['$92,368 '], 'Bonus__': ['0%'], '_version_': 1811884976682041344, '_root_': '80d33121-4502-4533-8679-17d46524ed64'}\n",
      "{'Department': ['Sales'], 'Gender': ['Male'], 'Ethnicity': ['Asian'], 'Age': [58], 'Country': ['United States'], 'City': ['Chicago'], 'id': '9f9a163b-236b-4175-ab42-b45a726c917f', 'Employee_ID': ['E02003'], 'Full_Name': ['Robert Patel'], 'Job_Title': ['Analyst'], 'Business_Unit': ['Corporate'], 'Hire_Date': ['10/23/2013'], 'Annual_Salary': ['$45,703 '], 'Bonus__': ['0%'], '_version_': 1811884976739713024, '_root_': '9f9a163b-236b-4175-ab42-b45a726c917f'}\n",
      "{'Department': ['IT'], 'Gender': ['Male'], 'Ethnicity': ['Asian'], 'Age': [34], 'Country': ['China'], 'City': ['Shanghai'], 'id': '19ab9cbc-81e8-453b-a709-6a73f5e05e3e', 'Employee_ID': ['E02004'], 'Full_Name': ['Cameron Lo'], 'Job_Title': ['Network Administrator'], 'Business_Unit': ['Research & Development'], 'Hire_Date': ['3/24/2019'], 'Annual_Salary': ['$83,576 '], 'Bonus__': ['0%'], '_version_': 1811884976741810176, '_root_': '19ab9cbc-81e8-453b-a709-6a73f5e05e3e'}\n",
      "{'Department': ['IT'], 'Gender': ['Female'], 'Ethnicity': ['Latino'], 'Age': [39], 'Country': ['United States'], 'City': ['Seattle'], 'id': '60837468-5f99-4801-b617-467e4b9ddf1c', 'Employee_ID': ['E02005'], 'Full_Name': ['Harper Castillo'], 'Job_Title': ['IT Systems Architect'], 'Business_Unit': ['Corporate'], 'Hire_Date': ['4/7/2018'], 'Annual_Salary': ['$98,062 '], 'Bonus__': ['0%'], '_version_': 1811884976742858752, '_root_': '60837468-5f99-4801-b617-467e4b9ddf1c'}\n",
      "{'Department': ['Engineering'], 'Gender': ['Female'], 'Ethnicity': ['Latino'], 'Age': [42], 'Country': ['United States'], 'City': ['Austin'], 'id': '5f08d247-e425-4321-bf61-32a83d216474', 'Employee_ID': ['E02006'], 'Full_Name': ['Harper Dominguez'], 'Job_Title': ['Director'], 'Business_Unit': ['Corporate'], 'Hire_Date': ['6/18/2005'], 'Annual_Salary': ['$175,391 '], 'Bonus__': ['24%'], '_version_': 1811884976743907328, '_root_': '5f08d247-e425-4321-bf61-32a83d216474'}\n",
      "{'Department': ['IT'], 'Gender': ['Male'], 'Ethnicity': ['Asian'], 'Age': [62], 'Country': ['United States'], 'City': ['Phoenix'], 'id': '8d421aac-c5a3-4ea4-a436-442d61933ff6', 'Employee_ID': ['E02007'], 'Full_Name': ['Ezra Vu'], 'Job_Title': ['Network Administrator'], 'Business_Unit': ['Manufacturing'], 'Hire_Date': ['4/22/2004'], 'Annual_Salary': ['$66,227 '], 'Bonus__': ['0%'], 'Exit_Date': ['2/14/2014'], '_version_': 1811884976800530432, '_root_': '8d421aac-c5a3-4ea4-a436-442d61933ff6'}\n",
      "{'Department': ['Accounting'], 'Gender': ['Female'], 'Ethnicity': ['Asian'], 'Age': [58], 'Country': ['China'], 'City': ['Chongqing'], 'id': '9b52a3d0-a150-4d42-8727-d6287f8ebad3', 'Employee_ID': ['E02008'], 'Full_Name': ['Jade Hu'], 'Job_Title': ['Sr. Analyst'], 'Business_Unit': ['Specialty Products'], 'Hire_Date': ['6/27/2009'], 'Annual_Salary': ['$89,744 '], 'Bonus__': ['0%'], '_version_': 1811884976802627584, '_root_': '9b52a3d0-a150-4d42-8727-d6287f8ebad3'}\n",
      "{'Department': ['Finance'], 'Gender': ['Male'], 'Ethnicity': ['Asian'], 'Age': [62], 'Country': ['China'], 'City': ['Chengdu'], 'id': '87731482-5b2d-42b8-ae33-89a7b3895779', 'Employee_ID': ['E02009'], 'Full_Name': ['Miles Chang'], 'Job_Title': ['Analyst II'], 'Business_Unit': ['Corporate'], 'Hire_Date': ['2/19/1999'], 'Annual_Salary': ['$69,674 '], 'Bonus__': ['0%'], '_version_': 1811884976803676160, '_root_': '87731482-5b2d-42b8-ae33-89a7b3895779'}\n",
      "{'Department': ['IT'], 'Gender': ['Female'], 'Ethnicity': ['Caucasian'], 'Age': [38], 'Country': ['United States'], 'City': ['Seattle'], 'id': '6661db22-0325-4adf-8035-d5fd3ca939b9', 'Employee_ID': ['E02010'], 'Full_Name': ['Gianna Holmes'], 'Job_Title': ['System Administratorï¿½'], 'Business_Unit': ['Manufacturing'], 'Hire_Date': ['9/9/2011'], 'Annual_Salary': ['$97,630 '], 'Bonus__': ['0%'], '_version_': 1811884976804724736, '_root_': '6661db22-0325-4adf-8035-d5fd3ca939b9'}\n",
      "{'Department': ['Finance'], 'Gender': ['Male'], 'Ethnicity': ['Caucasian'], 'Age': [52], 'Country': ['United States'], 'City': ['Miami'], 'id': 'c4eaa773-ba46-488a-8571-4de9e3663689', 'Employee_ID': ['E02011'], 'Full_Name': ['Jameson Thomas'], 'Job_Title': ['Manager'], 'Business_Unit': ['Specialty Products'], 'Hire_Date': ['2/5/2015'], 'Annual_Salary': ['$105,879 '], 'Bonus__': ['10%'], '_version_': 1811884976805773312, '_root_': 'c4eaa773-ba46-488a-8571-4de9e3663689'}\n"
     ]
    }
   ],
   "source": [
    "import pysolr\n",
    "\n",
    "# Correct Solr URL pointing to the employee_data core\n",
    "solr_url = 'http://localhost:8983/solr/employee_data'\n",
    "\n",
    "# Initialize Solr connection\n",
    "solr = pysolr.Solr(solr_url, always_commit=True)\n",
    "\n",
    "# Try performing a search to test the connection\n",
    "try:\n",
    "    results = solr.search('*:*')  # Search for all documents\n",
    "    for result in results:\n",
    "        print(result)\n",
    "except pysolr.SolrError as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def createCollection(p_collection_name):\n",
    "    url = f'http://localhost:8983/solr/admin/cores?action=CREATE&name={p_collection_name}&configSet=_default'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Collection {p_collection_name} created successfully.\")\n",
    "    else:\n",
    "        print(f\"Error creating collection {p_collection_name}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexData(p_collection_name, p_exclude_column, data_file):\n",
    "    # Load the CSV into a DataFrame\n",
    "    df = pd.read_csv(data_file)\n",
    "    \n",
    "    # Exclude the specified column\n",
    "    if p_exclude_column in df.columns:\n",
    "        df = df.drop(columns=[p_exclude_column])\n",
    "    \n",
    "    # Convert the DataFrame to a dictionary and add it to Solr\n",
    "    solr_url = f'http://localhost:8983/solr/{p_collection_name}'\n",
    "    solr = pysolr.Solr(solr_url, always_commit=True)\n",
    "    \n",
    "    solr.add(df.to_dict(orient='records'))\n",
    "    print(f\"Data indexed into collection {p_collection_name}, excluding {p_exclude_column}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchByColumn(p_collection_name, p_column_name, p_column_value):\n",
    "    solr_url = f'http://localhost:8983/solr/{p_collection_name}'\n",
    "    solr = pysolr.Solr(solr_url, always_commit=True)\n",
    "    \n",
    "    # Build the query\n",
    "    query = f'{p_column_name}:{p_column_value}'\n",
    "    results = solr.search(query)\n",
    "    \n",
    "    print(f\"Results for {p_column_name} = {p_column_value}:\")\n",
    "    for result in results:\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmpCount(p_collection_name):\n",
    "    solr_url = f'http://localhost:8983/solr/{p_collection_name}'\n",
    "    solr = pysolr.Solr(solr_url, always_commit=True)\n",
    "    \n",
    "    # Query to count all documents\n",
    "    results = solr.search('*:*')\n",
    "    \n",
    "    print(f\"Total employee count in collection {p_collection_name}: {results.hits}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delEmpById(p_collection_name, p_employee_id):\n",
    "    solr_url = f'http://localhost:8983/solr/{p_collection_name}'\n",
    "    solr = pysolr.Solr(solr_url, always_commit=True)\n",
    "    \n",
    "    solr.delete(id=p_employee_id)\n",
    "    print(f\"Deleted employee with ID {p_employee_id} from collection {p_collection_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDepFacet(p_collection_name):\n",
    "    solr_url = f'http://localhost:8983/solr/{p_collection_name}'\n",
    "    solr = pysolr.Solr(solr_url, always_commit=True)\n",
    "    \n",
    "    # Query with faceting by Department\n",
    "    facet_query = {'q': '*:*', 'facet': 'true', 'facet.field': 'Department'}\n",
    "    results = solr.search(q='*:*', **facet_query)\n",
    "    \n",
    "    # Print facet results\n",
    "    facet_counts = results.facets['facet_fields']['Department']\n",
    "    print(\"Employee count by Department:\")\n",
    "    for i in range(0, len(facet_counts), 2):\n",
    "        print(f\"{facet_counts[i]}: {facet_counts[i+1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection collection_2 created successfully.\n",
      "Collection 15323 created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define collection names\n",
    "v_nameCollection = 'collection_2'\n",
    "v_phoneCollection = '15323'\n",
    "\n",
    "# Create collections\n",
    "createCollection(v_nameCollection)\n",
    "createCollection(v_phoneCollection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total employee count in collection collection_2: 0\n"
     ]
    }
   ],
   "source": [
    "# Get initial employee count\n",
    "getEmpCount(v_nameCollection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa0 in position 1107: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Index data excluding Department for v_nameCollection and Gender for v_phoneCollection\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m indexData(v_nameCollection, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDepartment\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msolr\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124memployee_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m indexData(v_phoneCollection, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msolr\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124memployee_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m, in \u001b[0;36mindexData\u001b[1;34m(p_collection_name, p_exclude_column, data_file)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mindexData\u001b[39m(p_collection_name, p_exclude_column, data_file):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Load the CSV into a DataFrame\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_file)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Exclude the specified column\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p_exclude_column \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa0 in position 1107: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Index data excluding Department for v_nameCollection and Gender for v_phoneCollection\n",
    "indexData(v_nameCollection, 'Department', 'D:\\\\solr\\\\employee_data.csv')\n",
    "indexData(v_phoneCollection, 'Gender', 'D:\\\\solr\\\\employee_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
